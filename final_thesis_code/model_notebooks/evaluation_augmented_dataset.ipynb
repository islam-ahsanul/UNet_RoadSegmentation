{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "978ccaab",
   "metadata": {},
   "source": [
    "### Calculate the metrics\n",
    "\n",
    "1. F1\n",
    "2. Mean IoU\n",
    "3. Recall\n",
    "4. Precision\n",
    "5. Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c377a6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score, jaccard_score, recall_score, precision_score, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ff60ae",
   "metadata": {},
   "source": [
    "### Load predict mask and ground truth mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34eb51c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to predicted masks and ground truth masks\n",
    "pred_mask_dir = \"/home/ahsan/University/Thesis/UNet_Directory/Datasets/second_phase/predicted_masks/aug\"\n",
    "true_mask_dir = \"/home/ahsan/University/Thesis/UNet_Directory/Datasets/second_phase/processed_dataset/aug/test/masks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2a0721e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load mask file paths\n",
    "pred_mask_paths = sorted(glob(os.path.join(pred_mask_dir, \"*.png\")))\n",
    "true_mask_paths = sorted(glob(os.path.join(true_mask_dir, \"*.png\")))\n",
    "\n",
    "# Ensure equal number of files\n",
    "assert len(pred_mask_paths) == len(true_mask_paths), \"Mismatch in predicted and ground truth mask counts.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4beae51",
   "metadata": {},
   "source": [
    "### Calculate the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2041b28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  68%|██████▊   | 13/19 [00:20<00:09,  1.59s/it]/home/ahsan/PortableApps/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "Evaluating:  74%|███████▎  | 14/19 [00:22<00:07,  1.59s/it]/home/ahsan/PortableApps/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "Evaluating: 100%|██████████| 19/19 [00:30<00:00,  1.60s/it]\n"
     ]
    }
   ],
   "source": [
    "# Updated class mappings for grayscale values\n",
    "CLASS_MAPPING = {0: 0, 128: 1, 255: 2}\n",
    "VALID_VALUES = np.array([0, 128, 255])  # Expected grayscale values\n",
    "\n",
    "# Function to convert grayscale values to class indices\n",
    "def convert_mask(mask):\n",
    "    mask_converted = np.zeros_like(mask, dtype=np.uint8)\n",
    "    for gray_value, class_id in CLASS_MAPPING.items():\n",
    "        mask_converted[mask == gray_value] = class_id\n",
    "    return mask_converted\n",
    "\n",
    "# Initialize metrics\n",
    "scores = []\n",
    "\n",
    "# Metrics Calculation\n",
    "for pred_path, true_path in tqdm(zip(pred_mask_paths, true_mask_paths), total=len(pred_mask_paths), desc=\"Evaluating\"):\n",
    "    # Load predicted and true masks\n",
    "    pred_mask = cv2.imread(pred_path, cv2.IMREAD_GRAYSCALE)\n",
    "    true_mask = cv2.imread(true_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    if pred_mask is None or true_mask is None:\n",
    "        print(f\"Skipping {pred_path} or {true_path} due to loading error.\")\n",
    "        continue\n",
    "\n",
    "    # Validate mask values\n",
    "    if not np.all(np.isin(np.unique(pred_mask), VALID_VALUES)) or not np.all(np.isin(np.unique(true_mask), VALID_VALUES)):\n",
    "        print(f\"Unexpected values in {pred_path} or {true_path}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Convert grayscale mask values to class indices (0, 1, 2)\n",
    "    pred_mask = convert_mask(pred_mask).flatten().astype(np.int32)\n",
    "    true_mask = convert_mask(true_mask).flatten().astype(np.int32)\n",
    "\n",
    "    # Calculate metrics\n",
    "    acc = accuracy_score(true_mask, pred_mask)\n",
    "    f1 = f1_score(true_mask, pred_mask, average=\"macro\", labels=[0, 1, 2])\n",
    "    iou = jaccard_score(true_mask, pred_mask, average=\"macro\", labels=[0, 1, 2])\n",
    "    recall = recall_score(true_mask, pred_mask, average=\"macro\", labels=[0, 1, 2])\n",
    "    precision = precision_score(true_mask, pred_mask, average=\"macro\", labels=[0, 1, 2])\n",
    "\n",
    "    # Append metrics\n",
    "    scores.append([os.path.basename(pred_path), acc, f1, iou, recall, precision])\n",
    "\n",
    "# Check if any valid scores were collected\n",
    "if len(scores) == 0:\n",
    "    print(\"No valid masks evaluated.\")\n",
    "    exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1b05e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to numpy array for mean calculation\n",
    "scores_np = np.array([s[1:] for s in scores])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bbbe503c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_scores = np.mean(scores_np, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe6273c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.94043\n",
      "Mean F1 Score: 0.85517\n",
      "Mean IoU: 0.80693\n",
      "Mean Recall: 0.85777\n",
      "Mean Precision: 0.87467\n"
     ]
    }
   ],
   "source": [
    "# Print Metrics\n",
    "print(f\"Mean Accuracy: {mean_scores[0]:0.5f}\")\n",
    "print(f\"Mean F1 Score: {mean_scores[1]:0.5f}\")\n",
    "print(f\"Mean IoU: {mean_scores[2]:0.5f}\")\n",
    "print(f\"Mean Recall: {mean_scores[3]:0.5f}\")\n",
    "print(f\"Mean Precision: {mean_scores[4]:0.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1f8c7ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics saved to /home/ahsan/University/Thesis/UNet_Directory/Datasets/second_phase/files/aug/evaluation_results.csv\n"
     ]
    }
   ],
   "source": [
    "# Optional: Save detailed metrics to a file\n",
    "output_file = \"/home/ahsan/University/Thesis/UNet_Directory/Datasets/second_phase/files/aug/evaluation_results.csv\"\n",
    "with open(output_file, \"w\") as f:\n",
    "    f.write(\"Image,Accuracy,F1 Score,IoU,Recall,Precision\\n\")\n",
    "    for score in scores:\n",
    "        f.write(\",\".join(map(str, score)) + \"\\n\")\n",
    "    f.write(f\"\\nMean Metrics,,{mean_scores[0]:0.5f},{mean_scores[1]:0.5f},{mean_scores[2]:0.5f},{mean_scores[3]:0.5f},{mean_scores[4]:0.5f}\\n\")\n",
    "    \n",
    "print(f\"Metrics saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8586f858",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load evaluation results from CSV\n",
    "csv_path = \"/home/ahsan/University/Thesis/UNet_Directory/Datasets/second_phase/files/aug/evaluation_results.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Compute mean scores for visualization\n",
    "mean_scores = df.iloc[-1, 2:].values.astype(float)\n",
    "\n",
    "metrics = [\"Accuracy\", \"F1 Score\", \"IoU\", \"Recall\", \"Precision\"]\n",
    "values = list(mean_scores)\n",
    "\n",
    "# Bar chart for metric comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(metrics, values, color=['blue', 'green', 'orange', 'red', 'purple'])\n",
    "plt.ylim(0, 1)\n",
    "plt.xlabel(\"Metrics\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Evaluation Metrics\")\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "# plt.xticks(rotation=45)\n",
    "# Display the chart\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
