{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "978ccaab",
   "metadata": {},
   "source": [
    "### Calculate the metrics\n",
    "\n",
    "1. F1\n",
    "2. Mean IoU\n",
    "3. Recall\n",
    "4. Precision\n",
    "5. Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c377a6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score, jaccard_score, recall_score, precision_score, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ff60ae",
   "metadata": {},
   "source": [
    "### Load predict mask and ground truth mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34eb51c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to predicted masks and ground truth masks\n",
    "pred_mask_dir = \"/home/ahsan/University/Thesis/UNet_Directory/Datasets/second_phase/predicted_masks/aug\"\n",
    "true_mask_dir = \"/home/ahsan/University/Thesis/UNet_Directory/Datasets/mask_images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a0721e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load mask file paths\n",
    "pred_mask_paths = sorted(glob(os.path.join(pred_mask_dir, \"*.png\")))\n",
    "true_mask_paths = sorted(glob(os.path.join(true_mask_dir, \"*.png\")))\n",
    "\n",
    "# Ensure equal number of files\n",
    "assert len(pred_mask_paths) == len(true_mask_paths), \"Mismatch in predicted and ground truth mask counts.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4beae51",
   "metadata": {},
   "source": [
    "### Calculate the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2041b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize metrics\n",
    "scores = []\n",
    "\n",
    "# Metrics Calculation\n",
    "for pred_path, true_path in tqdm(zip(pred_mask_paths, true_mask_paths), total=len(pred_mask_paths), desc=\"Evaluating\"):\n",
    "    # Load predicted and true masks\n",
    "    pred_mask = cv2.imread(pred_path, cv2.IMREAD_GRAYSCALE)\n",
    "    true_mask = cv2.imread(true_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    if pred_mask is None or true_mask is None:\n",
    "        print(f\"Skipping {pred_path} or {true_path} due to loading error.\")\n",
    "        continue\n",
    "\n",
    "    # Flatten and ensure integer labels\n",
    "    pred_mask = pred_mask.flatten().astype(np.int32)\n",
    "    true_mask = true_mask.flatten().astype(np.int32)\n",
    "\n",
    "    # Ensure mask values are within the expected range\n",
    "    unique_pred = np.unique(pred_mask)\n",
    "    unique_true = np.unique(true_mask)\n",
    "    if not np.all(np.isin(unique_pred, [0, 1, 2])) or not np.all(np.isin(unique_true, [0, 1, 2])):\n",
    "        print(f\"Unexpected values in {pred_path}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Calculate metrics\n",
    "    acc = accuracy_score(true_mask, pred_mask)\n",
    "    f1 = f1_score(true_mask, pred_mask, average=\"macro\", labels=[0, 1, 2])\n",
    "    iou = jaccard_score(true_mask, pred_mask, average=\"macro\", labels=[0, 1, 2])\n",
    "    recall = recall_score(true_mask, pred_mask, average=\"macro\", labels=[0, 1, 2])\n",
    "    precision = precision_score(true_mask, pred_mask, average=\"macro\", labels=[0, 1, 2])\n",
    "\n",
    "    # Append metrics\n",
    "    scores.append([os.path.basename(pred_path), acc, f1, iou, recall, precision])\n",
    "    \n",
    "# Check if any valid scores were collected\n",
    "if len(scores) == 0:\n",
    "    print(\"No valid masks evaluated.\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b05e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to numpy array for mean calculation\n",
    "scores_np = np.array([s[1:] for s in scores])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbe503c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean metrics\n",
    "mean_scores = np.mean(scores_np, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6273c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Metrics\n",
    "print(f\"Mean Accuracy: {mean_scores[0]:0.5f}\")\n",
    "print(f\"Mean F1 Score: {mean_scores[1]:0.5f}\")\n",
    "print(f\"Mean IoU: {mean_scores[2]:0.5f}\")\n",
    "print(f\"Mean Recall: {mean_scores[3]:0.5f}\")\n",
    "print(f\"Mean Precision: {mean_scores[4]:0.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f8c7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Save detailed metrics to a file\n",
    "output_file = \"/home/ahsan/University/Thesis/UNet_Directory/Datasets/second_phase/files/aug/evaluation_results.csv\"\n",
    "with open(output_file, \"w\") as f:\n",
    "    f.write(\"Image,Accuracy,F1 Score,IoU,Recall,Precision\\n\")\n",
    "    for score in scores:\n",
    "        f.write(\",\".join(map(str, score)) + \"\\n\")\n",
    "    f.write(f\"\\nMean Metrics,,{mean_scores[0]:0.5f},{mean_scores[1]:0.5f},{mean_scores[2]:0.5f},{mean_scores[3]:0.5f},{mean_scores[4]:0.5f}\\n\")\n",
    "    \n",
    "print(f\"Metrics saved to {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
