{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new Ultralytics Settings v0.0.6 file ✅ \n",
      "View Ultralytics Settings with 'yolo settings' or at '/home/ahsan/.config/Ultralytics/settings.json'\n",
      "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seeding for reproducibility\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Hyperparameters\n",
    "height = 512\n",
    "width = 768\n",
    "num_classes = 3\n",
    "\n",
    "# Paths\n",
    "dataset_path = \"/home/ahsan/University/Thesis/UNet_Directory/Datasets/second_phase/processed_dataset/aug/test\"\n",
    "save_path = \"/home/ahsan/University/Thesis/UNet_Directory/Datasets/second_phase/combined_result\"\n",
    "yolo_model_path = \"/home/ahsan/University/Thesis/UNet_Directory/Datasets/second_phase/files/aug/best.pt\"\n",
    "unet_model_path = \"/home/ahsan/University/Thesis/UNet_Directory/Datasets/second_phase/files/aug/unet-multiclass.keras\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Save Directory\n",
    "os.makedirs(save_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load YOLO Model\n",
    "yolo_model = YOLO(yolo_model_path)\n",
    "\n",
    "# Load U-Net Model\n",
    "unet_model = tf.keras.models.load_model(unet_model_path, compile=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test images\n",
    "test_x = sorted(glob(os.path.join(dataset_path, \"images\", \"*.png\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images:   0%|          | 0/45 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /home/ahsan/University/Thesis/UNet_Directory/Datasets/second_phase/processed_dataset/aug/test/images/0008.png: 448x640 (no detections), 92.8ms\n",
      "Speed: 1.1ms preprocess, 92.8ms inference, 0.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 934ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images:   2%|▏         | 1/45 [00:01<00:53,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /home/ahsan/University/Thesis/UNet_Directory/Datasets/second_phase/processed_dataset/aug/test/images/0010.png: 448x640 1 person, 83.8ms\n",
      "Speed: 19.3ms preprocess, 83.8ms inference, 0.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 721ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images:   4%|▍         | 2/45 [00:02<00:43,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /home/ahsan/University/Thesis/UNet_Directory/Datasets/second_phase/processed_dataset/aug/test/images/0014.png: 448x640 (no detections), 88.3ms\n",
      "Speed: 0.9ms preprocess, 88.3ms inference, 0.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 726ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images:   7%|▋         | 3/45 [00:02<00:40,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /home/ahsan/University/Thesis/UNet_Directory/Datasets/second_phase/processed_dataset/aug/test/images/0016.png: 448x640 (no detections), 89.7ms\n",
      "Speed: 17.7ms preprocess, 89.7ms inference, 0.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 713ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images:   9%|▉         | 4/45 [00:03<00:37,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /home/ahsan/University/Thesis/UNet_Directory/Datasets/second_phase/processed_dataset/aug/test/images/0035.png: 448x640 (no detections), 85.9ms\n",
      "Speed: 0.8ms preprocess, 85.9ms inference, 0.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 723ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images:  11%|█         | 5/45 [00:04<00:36,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /home/ahsan/University/Thesis/UNet_Directory/Datasets/second_phase/processed_dataset/aug/test/images/0039.png: 448x640 (no detections), 79.2ms\n",
      "Speed: 18.2ms preprocess, 79.2ms inference, 0.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 711ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images:  13%|█▎        | 6/45 [00:05<00:34,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /home/ahsan/University/Thesis/UNet_Directory/Datasets/second_phase/processed_dataset/aug/test/images/0040.png: 448x640 (no detections), 88.2ms\n",
      "Speed: 0.8ms preprocess, 88.2ms inference, 0.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 726ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images:  16%|█▌        | 7/45 [00:06<00:33,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /home/ahsan/University/Thesis/UNet_Directory/Datasets/second_phase/processed_dataset/aug/test/images/0050.png: 448x640 1 person, 1 bicycle, 99.1ms\n",
      "Speed: 19.0ms preprocess, 99.1ms inference, 0.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 718ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images:  18%|█▊        | 8/45 [00:07<00:32,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /home/ahsan/University/Thesis/UNet_Directory/Datasets/second_phase/processed_dataset/aug/test/images/0057.png: 448x640 (no detections), 83.4ms\n",
      "Speed: 0.8ms preprocess, 83.4ms inference, 0.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 735ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images:  20%|██        | 9/45 [00:08<00:31,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /home/ahsan/University/Thesis/UNet_Directory/Datasets/second_phase/processed_dataset/aug/test/images/0060.png: 448x640 1 person, 1 rickshaw van, 85.3ms\n",
      "Speed: 18.7ms preprocess, 85.3ms inference, 0.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 719ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images:  22%|██▏       | 10/45 [00:09<00:30,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /home/ahsan/University/Thesis/UNet_Directory/Datasets/second_phase/processed_dataset/aug/test/images/0062.png: 448x640 1 person, 1 motorcycle, 85.2ms\n",
      "Speed: 0.9ms preprocess, 85.2ms inference, 0.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 765ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images:  24%|██▍       | 11/45 [00:09<00:30,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /home/ahsan/University/Thesis/UNet_Directory/Datasets/second_phase/processed_dataset/aug/test/images/0074.png: 448x640 (no detections), 86.2ms\n",
      "Speed: 17.8ms preprocess, 86.2ms inference, 0.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 721ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images:  27%|██▋       | 12/45 [00:11<00:34,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /home/ahsan/University/Thesis/UNet_Directory/Datasets/second_phase/processed_dataset/aug/test/images/0083.png: 448x640 (no detections), 82.0ms\n",
      "Speed: 19.0ms preprocess, 82.0ms inference, 0.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 717ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images:  29%|██▉       | 13/45 [00:12<00:31,  1.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /home/ahsan/University/Thesis/UNet_Directory/Datasets/second_phase/processed_dataset/aug/test/images/0084.png: 448x640 1 person, 1 motorcycle, 101.9ms\n",
      "Speed: 0.8ms preprocess, 101.9ms inference, 0.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 709ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images:  31%|███       | 14/45 [00:13<00:29,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /home/ahsan/University/Thesis/UNet_Directory/Datasets/second_phase/processed_dataset/aug/test/images/0093.png: 448x640 1 auto rickshaw, 1 private car, 82.4ms\n",
      "Speed: 18.0ms preprocess, 82.4ms inference, 0.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 716ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images:  33%|███▎      | 15/45 [00:14<00:32,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /home/ahsan/University/Thesis/UNet_Directory/Datasets/second_phase/processed_dataset/aug/test/images/0099.png: 448x640 1 rickshaw van, 82.2ms\n",
      "Speed: 18.4ms preprocess, 82.2ms inference, 0.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 705ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images:  36%|███▌      | 16/45 [00:15<00:29,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /home/ahsan/University/Thesis/UNet_Directory/Datasets/second_phase/processed_dataset/aug/test/images/0104.png: 448x640 (no detections), 85.0ms\n",
      "Speed: 0.8ms preprocess, 85.0ms inference, 0.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 717ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images:  38%|███▊      | 17/45 [00:16<00:27,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /home/ahsan/University/Thesis/UNet_Directory/Datasets/second_phase/processed_dataset/aug/test/images/0109.png: 448x640 (no detections), 83.3ms\n",
      "Speed: 17.8ms preprocess, 83.3ms inference, 0.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 724ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images:  40%|████      | 18/45 [00:17<00:25,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /home/ahsan/University/Thesis/UNet_Directory/Datasets/second_phase/processed_dataset/aug/test/images/0115.png: 448x640 (no detections), 82.4ms\n",
      "Speed: 0.8ms preprocess, 82.4ms inference, 0.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 720ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images:  42%|████▏     | 19/45 [00:18<00:23,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /home/ahsan/University/Thesis/UNet_Directory/Datasets/second_phase/processed_dataset/aug/test/images/0118.png: 448x640 4 persons, 1 rickshaw, 2 auto rickshaws, 89.6ms\n",
      "Speed: 18.5ms preprocess, 89.6ms inference, 0.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 731ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images:  44%|████▍     | 20/45 [00:18<00:22,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /home/ahsan/University/Thesis/UNet_Directory/Datasets/second_phase/processed_dataset/aug/test/images/0119.png: 448x640 1 person, 1 bicycle, 90.8ms\n",
      "Speed: 0.8ms preprocess, 90.8ms inference, 0.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 720ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images:  47%|████▋     | 21/45 [00:20<00:25,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /home/ahsan/University/Thesis/UNet_Directory/Datasets/second_phase/processed_dataset/aug/test/images/0120.png: 448x640 1 person, 1 auto rickshaw, 94.9ms\n",
      "Speed: 18.0ms preprocess, 94.9ms inference, 0.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 728ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images:  49%|████▉     | 22/45 [00:21<00:23,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /home/ahsan/University/Thesis/UNet_Directory/Datasets/second_phase/processed_dataset/aug/test/images/0121.png: 448x640 1 person, 1 auto rickshaw, 95.0ms\n",
      "Speed: 0.8ms preprocess, 95.0ms inference, 0.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 725ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images:  51%|█████     | 23/45 [00:22<00:21,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /home/ahsan/University/Thesis/UNet_Directory/Datasets/second_phase/processed_dataset/aug/test/images/0122.png: 448x640 2 persons, 82.9ms\n",
      "Speed: 19.0ms preprocess, 82.9ms inference, 0.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 733ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images:  53%|█████▎    | 24/45 [00:23<00:19,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /home/ahsan/University/Thesis/UNet_Directory/Datasets/second_phase/processed_dataset/aug/test/images/0123.png: 448x640 1 person, 94.7ms\n",
      "Speed: 0.8ms preprocess, 94.7ms inference, 0.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 724ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images:  56%|█████▌    | 25/45 [00:23<00:18,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /home/ahsan/University/Thesis/UNet_Directory/Datasets/second_phase/processed_dataset/aug/test/images/0124.png: 448x640 2 persons, 90.4ms\n",
      "Speed: 17.8ms preprocess, 90.4ms inference, 0.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 739ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images:  58%|█████▊    | 26/45 [00:25<00:20,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /home/ahsan/University/Thesis/UNet_Directory/Datasets/second_phase/processed_dataset/aug/test/images/0125.png: 448x640 1 person, 1 auto rickshaw, 1 truck, 85.8ms\n",
      "Speed: 17.9ms preprocess, 85.8ms inference, 0.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 715ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images:  60%|██████    | 27/45 [00:26<00:18,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /home/ahsan/University/Thesis/UNet_Directory/Datasets/second_phase/processed_dataset/aug/test/images/0126.png: 448x640 2 persons, 1 bicycle, 87.9ms\n",
      "Speed: 0.9ms preprocess, 87.9ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 724ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images:  62%|██████▏   | 28/45 [00:27<00:19,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /home/ahsan/University/Thesis/UNet_Directory/Datasets/second_phase/processed_dataset/aug/test/images/0127.png: 448x640 1 person, 1 auto rickshaw, 93.3ms\n",
      "Speed: 17.9ms preprocess, 93.3ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 720ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images:  64%|██████▍   | 29/45 [00:28<00:16,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /home/ahsan/University/Thesis/UNet_Directory/Datasets/second_phase/processed_dataset/aug/test/images/0128.png: 448x640 1 auto rickshaw, 1 motorcycle, 91.7ms\n",
      "Speed: 0.8ms preprocess, 91.7ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 716ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images:  67%|██████▋   | 30/45 [00:29<00:15,  1.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /home/ahsan/University/Thesis/UNet_Directory/Datasets/second_phase/processed_dataset/aug/test/images/0129.png: 448x640 1 auto rickshaw, 83.7ms\n",
      "Speed: 17.8ms preprocess, 83.7ms inference, 0.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 740ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images:  69%|██████▉   | 31/45 [00:30<00:13,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /home/ahsan/University/Thesis/UNet_Directory/Datasets/second_phase/processed_dataset/aug/test/images/0130.png: 448x640 2 auto rickshaws, 83.1ms\n",
      "Speed: 0.8ms preprocess, 83.1ms inference, 0.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 726ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images:  71%|███████   | 32/45 [00:31<00:12,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /home/ahsan/University/Thesis/UNet_Directory/Datasets/second_phase/processed_dataset/aug/test/images/0131.png: 448x640 2 auto rickshaws, 87.0ms\n",
      "Speed: 17.8ms preprocess, 87.0ms inference, 0.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 713ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images:  73%|███████▎  | 33/45 [00:32<00:11,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /home/ahsan/University/Thesis/UNet_Directory/Datasets/second_phase/processed_dataset/aug/test/images/0132.png: 448x640 2 persons, 1 auto rickshaw, 2 motorcycles, 2 buss, 86.8ms\n",
      "Speed: 0.9ms preprocess, 86.8ms inference, 0.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 720ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images:  76%|███████▌  | 34/45 [00:32<00:09,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /home/ahsan/University/Thesis/UNet_Directory/Datasets/second_phase/processed_dataset/aug/test/images/0133.png: 448x640 1 micro bus, 88.5ms\n",
      "Speed: 17.9ms preprocess, 88.5ms inference, 0.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 720ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images:  78%|███████▊  | 35/45 [00:33<00:08,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /home/ahsan/University/Thesis/UNet_Directory/Datasets/second_phase/processed_dataset/aug/test/images/0134.png: 448x640 1 person, 1 motorcycle, 1 micro bus, 86.5ms\n",
      "Speed: 0.9ms preprocess, 86.5ms inference, 0.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 721ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images:  80%|████████  | 36/45 [00:34<00:07,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /home/ahsan/University/Thesis/UNet_Directory/Datasets/second_phase/processed_dataset/aug/test/images/0135.png: 448x640 1 auto rickshaw, 1 micro bus, 90.1ms\n",
      "Speed: 17.7ms preprocess, 90.1ms inference, 0.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 714ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images:  82%|████████▏ | 37/45 [00:35<00:07,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /home/ahsan/University/Thesis/UNet_Directory/Datasets/second_phase/processed_dataset/aug/test/images/0136.png: 448x640 2 persons, 1 bus, 1 micro bus, 98.3ms\n",
      "Speed: 0.8ms preprocess, 98.3ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 713ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images:  84%|████████▍ | 38/45 [00:36<00:06,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /home/ahsan/University/Thesis/UNet_Directory/Datasets/second_phase/processed_dataset/aug/test/images/0137.png: 448x640 1 person, 2 auto rickshaws, 1 bus, 2 micro buss, 83.8ms\n",
      "Speed: 17.8ms preprocess, 83.8ms inference, 0.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 744ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images:  87%|████████▋ | 39/45 [00:37<00:05,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /home/ahsan/University/Thesis/UNet_Directory/Datasets/second_phase/processed_dataset/aug/test/images/0138.png: 448x640 1 auto rickshaw, 2 private cars, 3 micro buss, 80.5ms\n",
      "Speed: 0.9ms preprocess, 80.5ms inference, 0.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 732ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images:  89%|████████▉ | 40/45 [00:38<00:04,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /home/ahsan/University/Thesis/UNet_Directory/Datasets/second_phase/processed_dataset/aug/test/images/0139.png: 448x640 1 auto rickshaw, 1 private car, 2 buss, 1 covered van, 87.5ms\n",
      "Speed: 18.5ms preprocess, 87.5ms inference, 0.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 721ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images:  91%|█████████ | 41/45 [00:39<00:04,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /home/ahsan/University/Thesis/UNet_Directory/Datasets/second_phase/processed_dataset/aug/test/images/0140.png: 448x640 1 person, 1 private car, 2 bicycles, 2 micro buss, 94.3ms\n",
      "Speed: 18.7ms preprocess, 94.3ms inference, 0.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 709ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images:  93%|█████████▎| 42/45 [00:40<00:02,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /home/ahsan/University/Thesis/UNet_Directory/Datasets/second_phase/processed_dataset/aug/test/images/0141.png: 448x640 2 persons, 1 private car, 1 motorcycle, 1 bicycle, 93.8ms\n",
      "Speed: 0.8ms preprocess, 93.8ms inference, 0.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 731ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images:  96%|█████████▌| 43/45 [00:41<00:01,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /home/ahsan/University/Thesis/UNet_Directory/Datasets/second_phase/processed_dataset/aug/test/images/0142.png: 448x640 1 auto rickshaw, 1 micro bus, 93.3ms\n",
      "Speed: 18.0ms preprocess, 93.3ms inference, 0.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 720ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images:  98%|█████████▊| 44/45 [00:42<00:01,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /home/ahsan/University/Thesis/UNet_Directory/Datasets/second_phase/processed_dataset/aug/test/images/0143.png: 448x640 1 micro bus, 83.3ms\n",
      "Speed: 18.0ms preprocess, 83.3ms inference, 0.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 715ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images: 100%|██████████| 45/45 [00:43<00:00,  1.03it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Mapping grayscale values to class labels for segmentation mask\n",
    "class_colors = {\n",
    "    0: (0, 0, 0),       # Non-Drivable Area (black)\n",
    "    1: (79, 247, 211),  # My Way (Greenish-Yellow)\n",
    "    2: (247, 93, 79)    # Other Way (Red)\n",
    "}\n",
    "\n",
    "# Track inference times\n",
    "time_taken = []\n",
    "\n",
    "for x_path in tqdm(test_x, desc=\"Processing Images\"):\n",
    "    name = os.path.basename(x_path)\n",
    "    original_img = cv2.imread(x_path, cv2.IMREAD_COLOR)\n",
    "\n",
    "    if original_img is None:\n",
    "        print(f\"Error loading image {x_path}, skipping.\")\n",
    "        continue\n",
    "\n",
    "    # Resize image for model input\n",
    "    resized_img = cv2.resize(original_img, (width, height))\n",
    "    img_input = resized_img / 255.0\n",
    "    img_input = np.expand_dims(img_input, axis=0)\n",
    "\n",
    "    ####### 1. YOLO Object Detection #######\n",
    "    yolo_results = yolo_model(x_path)[0]\n",
    "    yolo_output = original_img.copy()\n",
    "\n",
    "    for result in yolo_results.boxes:\n",
    "        x1, y1, x2, y2 = map(int, result.xyxy[0].tolist())  # Bounding box coordinates\n",
    "        confidence = result.conf[0].item()\n",
    "        label = int(result.cls[0].item())\n",
    "\n",
    "        # Draw bounding boxes\n",
    "        cv2.rectangle(yolo_output, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        cv2.putText(yolo_output, f\"Class {label}: {confidence:.2f}\", \n",
    "                    (x1, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "    # Save YOLO output image\n",
    "    yolo_save_path = os.path.join(save_path, f\"{name.split('.')[0]}_yolo.png\")\n",
    "    cv2.imwrite(yolo_save_path, yolo_output)\n",
    "\n",
    "    ####### 2. U-Net Road Segmentation #######\n",
    "    start_time = time.time()\n",
    "    unet_prediction = unet_model.predict(img_input)[0]\n",
    "    time_taken.append(time.time() - start_time)\n",
    "\n",
    "    # Convert model output to mask\n",
    "    predicted_mask = np.argmax(unet_prediction, axis=-1).astype(np.uint8)\n",
    "\n",
    "    # Convert mask to color format\n",
    "    seg_colored = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "    for label, color in class_colors.items():\n",
    "        seg_colored[predicted_mask == label] = color\n",
    "\n",
    "    # Save segmentation mask\n",
    "    seg_save_path = os.path.join(save_path, f\"{name.split('.')[0]}_segmentation.png\")\n",
    "    cv2.imwrite(seg_save_path, seg_colored)\n",
    "\n",
    "    ####### 3. Combined Image (YOLO + Segmentation) #######\n",
    "    combined_output = cv2.addWeighted(yolo_output, 0.6, seg_colored, 0.4, 0)\n",
    "\n",
    "    # Save combined output\n",
    "    combined_save_path = os.path.join(save_path, f\"{name.split('.')[0]}_combined.png\")\n",
    "    cv2.imwrite(combined_save_path, combined_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean time taken per image: 0.8297 seconds\n",
      "Mean FPS: 1.21\n"
     ]
    }
   ],
   "source": [
    "# FPS Calculation\n",
    "mean_time = np.mean(time_taken)\n",
    "mean_fps = 1 / mean_time\n",
    "print(f\"Mean time taken per image: {mean_time:.4f} seconds\")\n",
    "print(f\"Mean FPS: {mean_fps:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
