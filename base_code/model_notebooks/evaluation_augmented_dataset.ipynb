{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "978ccaab",
   "metadata": {},
   "source": [
    "### Calculate the metrics\n",
    "\n",
    "1. F1\n",
    "2. Mean IoU\n",
    "3. Recall\n",
    "4. Precision\n",
    "5. Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c377a6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score, jaccard_score, recall_score, precision_score, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ff60ae",
   "metadata": {},
   "source": [
    "### Load predict mask and ground truth mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34eb51c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_mask = \"/home/ahsan/University/Thesis/UNet_Directory/UNet_RoadSegmentation/model_notebooks/pred_masks\"\n",
    "true_mask = \"/home/ahsan/University/Thesis/UNet_Directory/UNet_RoadSegmentation/model_notebooks/true_masks\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a0721e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pred_mask[0], true_mask[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4beae51",
   "metadata": {},
   "source": [
    "### Calculate the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2041b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = []\n",
    "\n",
    "for pred_y, true_y in tqdm(zip(pred_mask, true_mask), total=len(pred_mask)):\n",
    "    name = pred_y.split(\"/\")[-1]\n",
    "    pred_y = cv2.imread(pred_y, cv2.IMREAD_GRAYSCALE)\n",
    "    pred_y = pred_y / 255\n",
    "    pred_y = pred_y > 0.5\n",
    "    pred_y = pred_y.astype(np.int32)\n",
    "    pred_y = pred_y.flatten()\n",
    "    \n",
    "    true_y = cv2.imread(true_y, cv2.IMREAD_GRAYSCALE)\n",
    "    true_y = true_y / 255\n",
    "    true_y = true_y > 0.5\n",
    "    true_y = true_y.astype(np.int32)\n",
    "    true_y = true_y.flatten()\n",
    "\n",
    "    acc_value = accuracy_score(true_y, pred_y)\n",
    "    f1_value = f1_score(true_y, pred_y, labels=[0, 1], average=\"binary\")\n",
    "    jac_value = jaccard_score(pred_y, true_y, labels=[0, 1], average=\"binary\")\n",
    "    recall_value = recall_score(pred_y, true_y, labels=[0, 1], average=\"binary\")\n",
    "    precision_value = precision_score(pred_y, true_y, labels=[0, 1], average=\"binary\")\n",
    "    \n",
    "    score.append([name, acc_value, f1_value, jac_value, recall_value, precision_value])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b05e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = [s[1:] for s in score]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbe503c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_score = np.mean(score, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6273c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Accuracy: {mean_score[0]:0.5f}\")\n",
    "print(f\"F1 Score: {mean_score[1]:0.5f}\")\n",
    "print(f\"Jaccard Score: {mean_score[2]:0.5f}\")\n",
    "print(f\"Recall Score: {mean_score[3]:0.5f}\")\n",
    "print(f\"Precision Score: {mean_score[4]:0.5f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
